{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Assignment 4 - Simple Linear vs. Ridge Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the historical heart of Boston, Bob seeks to understand the intricacies of the real estate market. With a linear regression model at his side, Bob wonders if he can improve his predictions. Given your expertise in machine learning, he turns to you for guidance. Specifically, he wants to unravel the factors influencing the median value of homes across different Boston neighborhoods.\n",
    "\n",
    "To assist Bob, you decide to:\n",
    "*  Implement the closed-form solution for linear regression. \n",
    "* Apply a polynomial transformation to increase model flexibility.\n",
    "* Utilize ridge regression to control model complexity.\n",
    "* Apply 10-fold cross-validation for more reliable performance estimates.\n",
    "\n",
    "\n",
    "Bob is curious and wants to see a comparison between linear and ridge regression, both with and without polynomial transformations, on the same dataset. Thus, the challenge begins!\n",
    "\n",
    " Variables in order:\n",
    "* CRIM:     per capita crime rate by town\n",
    "*  ZN:       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "* INDUS:    proportion of non-retail business acres per town\n",
    "* CHAS:     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "* NOX:      nitric oxides concentration (parts per 10 million)\n",
    "* RM:       average number of rooms per dwelling\n",
    "* AGE:      proportion of owner-occupied units built prior to 1940\n",
    "* DIS:      weighted distances to five Boston employment centres\n",
    "* RAD:      index of accessibility to radial highways\n",
    "* TAX:      full-value property-tax rate per \\$10,000\n",
    "* PTRATIO:  pupil-teacher ratio by town\n",
    "* B:        $1000(Bk - 0.63)^2$ where Bk is the proportion of blacks by town\n",
    "* LSTAT:    \\% lower status of the population\n",
    "* MEDV:     Median value of owner-occupied homes in \\$1000's\n",
    "\n",
    "Note: The Boston Housing dataset, especially the 'B' variable, touches upon serious ethical and societal concerns related to race and inequality. Reflect upon these issues, and consider strategies such as excluding the 'B' column from analyses.\n",
    "\n",
    "With this context, let's assist Bob in his real estate endeavors!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Setup and Data Preparation\n",
    "Import Libraries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # Fundamental package for linear algebra and multidimensional arrays\n",
    "import pandas as pd  # Data analysis and manipulation tool\n",
    "\n",
    "# Transform features to polynomial features for model flexibility\n",
    "from sklearn.preprocessing import PolynomialFeatures  \n",
    "\n",
    "# Split arrays or matrices into random train and test subsets\n",
    "from sklearn.model_selection import train_test_split  \n",
    "\n",
    "# Scale features to zero mean and unit variance, commonly used for normalization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Provides train/test indices to split data into train/test sets while performing cross-validation\n",
    "from sklearn.model_selection import KFold  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (506, 14)\n",
      "        0     1     2   3      4      5     6       7   8      9     10  \\\n",
      "0  0.00632  18.0  2.31   0  0.538  6.575  65.2  4.0900   1  296.0  15.3   \n",
      "1  0.02731   0.0  7.07   0  0.469  6.421  78.9  4.9671   2  242.0  17.8   \n",
      "2  0.02729   0.0  7.07   0  0.469  7.185  61.1  4.9671   2  242.0  17.8   \n",
      "3  0.03237   0.0  2.18   0  0.458  6.998  45.8  6.0622   3  222.0  18.7   \n",
      "4  0.06905   0.0  2.18   0  0.458  7.147  54.2  6.0622   3  222.0  18.7   \n",
      "\n",
      "       11    12    13  \n",
      "0  396.90  4.98  24.0  \n",
      "1  396.90  9.14  21.6  \n",
      "2  392.83  4.03  34.7  \n",
      "3  394.63  2.94  33.4  \n",
      "4  396.90  5.33  36.2  \n",
      "\n",
      "First 5 rows of X:\n",
      " [[6.3200e-03 1.8000e+01 2.3100e+00 0.0000e+00 5.3800e-01 6.5750e+00\n",
      "  6.5200e+01 4.0900e+00 1.0000e+00 2.9600e+02 1.5300e+01 3.9690e+02\n",
      "  4.9800e+00]\n",
      " [2.7310e-02 0.0000e+00 7.0700e+00 0.0000e+00 4.6900e-01 6.4210e+00\n",
      "  7.8900e+01 4.9671e+00 2.0000e+00 2.4200e+02 1.7800e+01 3.9690e+02\n",
      "  9.1400e+00]\n",
      " [2.7290e-02 0.0000e+00 7.0700e+00 0.0000e+00 4.6900e-01 7.1850e+00\n",
      "  6.1100e+01 4.9671e+00 2.0000e+00 2.4200e+02 1.7800e+01 3.9283e+02\n",
      "  4.0300e+00]\n",
      " [3.2370e-02 0.0000e+00 2.1800e+00 0.0000e+00 4.5800e-01 6.9980e+00\n",
      "  4.5800e+01 6.0622e+00 3.0000e+00 2.2200e+02 1.8700e+01 3.9463e+02\n",
      "  2.9400e+00]\n",
      " [6.9050e-02 0.0000e+00 2.1800e+00 0.0000e+00 4.5800e-01 7.1470e+00\n",
      "  5.4200e+01 6.0622e+00 3.0000e+00 2.2200e+02 1.8700e+01 3.9690e+02\n",
      "  5.3300e+00]]\n",
      "First 5 values of y:\n",
      " [[24. ]\n",
      " [21.6]\n",
      " [34.7]\n",
      " [33.4]\n",
      " [36.2]]\n",
      "X shape: (506, 13)\n",
      "y shape: (506, 1)\n"
     ]
    }
   ],
   "source": [
    "# Define feature names\n",
    "# Specifying the names of the columns in our dataset makes it easier to understand and reference them.\n",
    "feature_names = [\"CRIM\", \"ZN\", \"INDUS\", \"CHAS\", \"NOX\", \"RM\", \"AGE\", \"RAD\", \"PTRATIO\", \"B\", \"LSTAT\", \"MEDV\"]\n",
    "\n",
    "# Load the data\n",
    "# We read data from a CSV (Comma-Separated Values) file into a DataFrame. DataFrame is a 2D labeled data structure in pandas.\n",
    "filename = 'Boston_housing.csv'\n",
    "df = pd.read_csv(filename, sep='\\s+', header=None)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "# It's good practice to inspect the dataset's size and first few rows to ensure it's loaded correctly and understand its structure.\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(df.head())\n",
    "\n",
    "# Extract features and target\n",
    "# Machine learning typically involves using features (independent variables) to predict a target (dependent variable).\n",
    "# Here, we separate the dataset into features (X) and target (y).\n",
    "X = np.array(df.iloc[:, :13])  # All columns up to the 13th are features\n",
    "y = np.array(df.iloc[:, 13]).reshape(-1, 1)  # The 13th column is our target, and we reshape it to a 2D array for compatibility.\n",
    "\n",
    "# Preview data\n",
    "# It's also good practice to preview the data after separation to ensure everything looks as expected.\n",
    "print(\"\\nFirst 5 rows of X:\\n\", X[:5])\n",
    "print(\"First 5 values of y:\\n\", y[:5])\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for missing values\n",
    "\n",
    "After getting the data, it's always a good practice to check for missing values in the dataset. Luckily for us, this dataset has no missing values. Here's how you can verify that:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in X: 0\n",
      "Missing values in y: 0\n"
     ]
    }
   ],
   "source": [
    "# 2. Check for Missing Values:\n",
    "print(\"Missing values in X:\", np.isnan(X).sum())\n",
    "print(\"Missing values in y:\", np.isnan(y).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing 10-Fold Cross-Validation\n",
    "With the data now loaded into X and y, your next task is to implement the code to select the optimal regularization and polynomial transformation. Utilize 10-fold cross-validation to assess the various configurations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10-Fold Cross-Validation with Feature Scaling and Polynomial Transformation\n",
    "\n",
    "Cross-validation is a method to assess the performance of a machine learning model on unseen data by dividing the data into a set number of groups, or \"folds\".\n",
    "\n",
    "### Why 10-Fold Cross-Validation?\n",
    "\n",
    "In 10-fold cross-validation, the dataset is randomly divided into ten parts or folds. The idea is to iteratively train the model on 9 of these folds and test it on the tenth. This is done ten times, once for each fold acting as the validation set. By doing so, we're ensuring that each data point gets to be in a validation set exactly once.\n",
    "\n",
    "### Feature Scaling Within Cross-Validation\n",
    "\n",
    "Feature scaling ensures that all features contribute equally to the model performance, which is particularly important for algorithms sensitive to feature magnitudes.\n",
    "\n",
    "When doing cross-validation, it's crucial that we don't introduce data leakage by scaling using statistics from the entire dataset. Instead:\n",
    "1. Divide the data into training and validation sets.\n",
    "2. Fit the scaler on the training set.\n",
    "3. Apply the scaling to both the training and validation sets using this scaler.\n",
    "\n",
    "### Polynomial Transformation Within Cross-Validation\n",
    "\n",
    "Polynomial transformations capture more intricate data relationships by adding polynomial features. Here's how you incorporate it into cross-validation:\n",
    "1. Divide the data into training and validation sets.\n",
    "2. Fit the polynomial transformer on the training set.\n",
    "3. Transform both the training and validation sets using this transformer.\n",
    "4. Fit the scaler on the transformed training set\n",
    "4. Apply the scaling to both the transformed training and transformed validation sets using this scaler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Note on Cross-Validation Error Calculation\n",
    "\n",
    "In most lecture notes and literature on k-fold cross-validation, the procedure for calculating the cross-validation error typically involves computing the mean of the errors obtained from each fold. However, in the context of our analysis, given the relatively small size of the dataset and the possibility of unequal numbers of samples in each fold, this traditional approach might not be mathematically rigorous.\n",
    "\n",
    "To address this, our approach for calculating the cross-validation error will deviate slightly from the traditional method. Instead of merely averaging the errors from each fold, we will sum up the errors across all folds and then divide by $ N $, the total number of training examples. This ensures that our error estimate is unbiased and takes into account the potential discrepancy in the number of samples across different folds.\n",
    "\n",
    "Mathematically, the cross-validation error, $ E_{cv} $, for this assignment is computed as:\n",
    "$$  E_{\\text{cv}} = \\frac{1}{N} \\sum_{i=1}^{k} \\sum_{j \\in \\text{fold } i} (y^{(j)}- \\hat{y}^{(j)})^2\n",
    " $$\n",
    "where $ k $ is the number of folds, $ y^{(j)} $ is the true target value of the $j^{th} $ example, and $ \\hat{y}^{(j)} $ is the predicted value for the same example.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your code goes here\n",
    "\n",
    "Feel free to add any helper functions you may need.\n",
    "\n",
    "### Part a) 5-fold Cross Validation using Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(X, y):\n",
    "    # use np.linalg.pinv(a)    \n",
    "    # Compute the weights using the closed-form solution \n",
    "    #### TO-DO #####\n",
    "    w = np.linalg.pinv(X.T @ X) @ X.T @ y\n",
    "    \n",
    "    \n",
    "    ##############\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Next implement Squared Error. It measures the average squared difference between the estimated values (predictions) and the actual values (true values). Mathematically, it is represented as: $  \\sum_{i=1}^{N} (y^{(i)} - \\hat{y}^{(i)})^2 $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_error(y_true, y_pred):    \n",
    "    #### TO-DO ##### \n",
    "    # Calculate the squared differences\n",
    "    error = (y_true - y_pred) ** 2\n",
    "\n",
    "\n",
    "    ##############    \n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_linear_regression(X, y, k=10):\n",
    "    \"\"\"\n",
    "    Perform k-fold cross-validation for linear regression.\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=k, random_state=10, shuffle=True)\n",
    "    #### TO-DO #####\n",
    "    \n",
    "    \n",
    "    for train_index, val_index in kf.split(X):\n",
    "        X_train, X_val = X[train_index], X[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "        w = linear_regression(X_train, y_train)\n",
    "        \n",
    "        y_pred = w.T @ X\n",
    "        \n",
    "        print(y_pred)\n",
    "\n",
    "        # Fit the model on training data\n",
    "        \n",
    "        \n",
    "        # Calculate in-sample error and cross-validation error\n",
    "\n",
    "        \n",
    "    ##############    \n",
    "    return e_in, e_cv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use your code to answer question a)    \n",
    "#### TO-DO #####\n",
    "    \n",
    "    \n",
    "##############\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part b) Adding Ridge Regression\n",
    "Enhance the previous code to include Ridge Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(X, y, alpha):\n",
    "    # use np.linalg.pinv(a)    \n",
    "    # Compute the weights using the closed-form solution \n",
    "    #### TO-DO #####\n",
    "    \n",
    "    ##############\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def k_fold_ridge_regression(X, y, k=10, lambdas=[1e-4, 1e-3, 1e-2, 1e-1, 1.0]):\n",
    "    \"\"\"\n",
    "    Perform k-fold cross-validation for ridge regression with various lambda values.\n",
    "    \"\"\"\n",
    "    best_alpha = None\n",
    "    best_error = float('inf')\n",
    "    \n",
    "    kf = KFold(n_splits=k, random_state=10, shuffle=True)\n",
    "    \n",
    "    for alpha in lambdas:\n",
    "    #### TO-DO #####\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "            \n",
    "    ##############\n",
    "    return best_alpha, best_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use your code to answer question b)    \n",
    "#### TO-DO #####\n",
    "    \n",
    "    \n",
    "##############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part c) Adding Polynomial Transformations and Ridge Regression\n",
    "Extend their code to incorporate polynomial transformations combined with Ridge Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "def k_fold_poly_ridge(X, y, k=10, lambdas=[1e-4, 1e-3, 1e-2, 1e-1, 1.0, 2, 3, 4, 5], degrees=[1, 2, 3]):\n",
    "    \"\"\"\n",
    "    Perform k-fold cross-validation for ridge regression with various lambda values and polynomial transformations.\n",
    "    \"\"\"\n",
    "    best_lambda = None\n",
    "    best_degree = None\n",
    "    best_error = float('inf')\n",
    "    \n",
    "    kf = KFold(n_splits=k, random_state=10, shuffle=True)\n",
    "    \n",
    "    for degree in degrees:\n",
    "        poly = PolynomialFeatures(degree=degree)\n",
    "        X_poly = poly.fit_transform(X)\n",
    "    #### TO-DO #####\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ##############\n",
    "    return best_lambda, best_degree, best_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use your code to answer question b)    \n",
    "#### TO-DO #####\n",
    "    \n",
    "    \n",
    "##############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additonal code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
